# Words-Embedding-in-TensorFlow-2.0

In word embedding, each word is a unit. Each word is represented by some real-valued vector. The values of the vector of each word are learned in a way that similar words have similar kinds of vector values. It is because it is also considering the context of the word and relationship of words to each other. These vector values are learned in a deep neural fashion.

A word embedding is a learned representation for text where semantically similar words have a similar representation of a vector.

One of the benefits of using dense and low-dimensional vectors is computational: the majority of neural network toolkits do not play well with very high-dimensional, sparse vectors. … The main benefit of the dense representations is generalization power: if we believe some features may provide similar clues, it is worthwhile to provide a representation that is able to capture these similarities.


